
Epoch 00013: reducing learning rate of group 0 to 5.0000e-05.
Epoch 00019: reducing learning rate of group 0 to 2.5000e-05.
Epoch 00025: reducing learning rate of group 0 to 1.2500e-05.
Epoch 00031: reducing learning rate of group 0 to 6.2500e-06.
Epoch 00037: reducing learning rate of group 0 to 3.1250e-06.
Epoch 00043: reducing learning rate of group 0 to 1.5625e-06.
Traceback (most recent call last):
  File "C:\Users\beaviv\DIAMOND\SNDlib_traffic_prediction\SNDlib_run.py", line 135, in <module>
    run(model_name=model_name, training=True, test=False)
  File "C:\Users\beaviv\DIAMOND\SNDlib_traffic_prediction\SNDlib_run.py", line 124, in run
    train_loss, validation_loss, best_model = train(**train_hyperparameters)
  File "C:\Users\beaviv\DIAMOND\SNDlib_traffic_prediction\SNDlib_train.py", line 79, in train
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping
  File "C:\Users\beaviv\DIAMOND\venv\lib\site-packages\torch\nn\utils\clip_grad.py", line 43, in clip_grad_norm_
    total_norm = torch.norm(torch.stack([torch.norm(g.detach(), norm_type).to(device) for g in grads]), norm_type)
  File "C:\Users\beaviv\DIAMOND\venv\lib\site-packages\torch\nn\utils\clip_grad.py", line 43, in <listcomp>
    total_norm = torch.norm(torch.stack([torch.norm(g.detach(), norm_type).to(device) for g in grads]), norm_type)
  File "C:\Users\beaviv\DIAMOND\venv\lib\site-packages\torch\functional.py", line 1485, in norm
    return _VF.norm(input, p, dim=_dim, keepdim=keepdim)  # type: ignore[attr-defined]
KeyboardInterrupt