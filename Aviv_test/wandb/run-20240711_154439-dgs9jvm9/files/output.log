
Epoch 00008: reducing learning rate of group 0 to 2.5000e-04.
Epoch 10/100, Train Loss: 198.8325 , Validation Loss: 499.9132
Epoch 00014: reducing learning rate of group 0 to 1.2500e-04.
Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.
Epoch 20/100, Train Loss: 185.0652 , Validation Loss: 554.6814
Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.
Epoch 30/100, Train Loss: 183.1647 , Validation Loss: 582.1560
Epoch 00032: reducing learning rate of group 0 to 1.5625e-05.
Epoch 00038: reducing learning rate of group 0 to 7.8125e-06.
Epoch 40/100, Train Loss: 181.4652 , Validation Loss: 638.2915
Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.
Traceback (most recent call last):
  File "C:\Users\beaviv\DIAMOND\Aviv_test\Traffic_prediction_model.py", line 397, in <module>
    if __name__ == "__main__":
  File "C:\Users\beaviv\DIAMOND\Aviv_test\Traffic_prediction_model.py", line 392, in NTP
  File "C:\Users\beaviv\DIAMOND\Aviv_test\Traffic_prediction_model.py", line 186, in train
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping
  File "C:\Users\beaviv\DIAMOND\venv\lib\site-packages\torch\optim\optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "C:\Users\beaviv\DIAMOND\venv\lib\site-packages\torch\optim\optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "C:\Users\beaviv\DIAMOND\venv\lib\site-packages\torch\optim\adam.py", line 234, in step
    adam(params_with_grad,
  File "C:\Users\beaviv\DIAMOND\venv\lib\site-packages\torch\optim\adam.py", line 300, in adam
    func(params,
  File "C:\Users\beaviv\DIAMOND\venv\lib\site-packages\torch\optim\adam.py", line 354, in _single_tensor_adam
    grad = grad.add(param, alpha=weight_decay)
KeyboardInterrupt